% WARNING: 
% this file is generated by a script
% manual changes will likely be overwritten 
%
% nag when using outdated macros
\RequirePackage[l2tabu,orthodox]{nag}
\documentclass
[12pt,letterpaper]
{article}

% SETUP

% {{{ etex }}}
\usepackage{etex}
% {{{ common }}}
\usepackage{xspace,enumerate}
\usepackage[dvipsnames]{xcolor}
\usepackage[T1]{fontenc}
\usepackage[full]{textcomp}
% {{{ babelamerican }}}
\usepackage[american]{babel}
% {{{ mathtools }}}
\usepackage{mathtools}
% {{{ boldmath }}}
% fix for "too many math alphabets" problem
\newcommand\hmmax{0} % default 3
% \usepackage{bm}
% \usepackage{stmaryrd}
% {{{ amsthm }}}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem{subclaim}{Claim}[theorem]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem*{proposition*}{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{lemma*}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem*{conjecture*}{Conjecture}
\newtheorem{fact}[theorem]{Fact}
\newtheorem*{fact*}{Fact}
\newtheorem{hypothesis}[theorem]{Hypothesis}
\newtheorem*{hypothesis*}{Hypothesis}
\newtheorem{conjecture}[theorem]{Conjecture}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem*{definition*}{Definition}
\newtheorem{construction}[theorem]{Construction}
\newtheorem{example}[theorem]{Example}
\newtheorem{question}[theorem]{Question}
\newtheorem{openquestion}[theorem]{Open Question}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{protocol}[theorem]{Protocol}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{claim}[theorem]{Claim}
\newtheorem*{claim*}{Claim}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{remark*}{Remark}
\newtheorem{observation}[theorem]{Observation}
\newtheorem*{observation*}{Observation}
% {{{ geometry-nice }}}
\usepackage[
letterpaper,
top=0.8in,
bottom=0.8in,
left=1in,
right=1in]{geometry}
% {{{ newpx }}}
\usepackage{newpxtext} % T1, lining figures in math, osf in text
\usepackage{textcomp} % required for special glyphs
\usepackage[varg,bigdelims]{newpxmath}
\usepackage[scr=rsfso]{mathalfa}% \mathscr is fancier than \mathcal
\usepackage{bm} % load after all math to give access to bold math
% \useosf %no longer needed
\linespread{1.1}% Give Palatino more leading (space between lines)
\let\mathbb\varmathbb
% {{{ microtype }}}
\usepackage{microtype}
\usepackage[
pagebackref,
% letterpaper=true,
colorlinks=true,
urlcolor=blue,
linkcolor=blue,
citecolor=OliveGreen,
]{hyperref}
% load amsthm here already
\usepackage{amsthm}
\usepackage[capitalise,nameinlink]{cleveref}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{question}{Question}{Questions}
\crefname{conjecture}{Conjecture}{Conjectures}
\crefname{fact}{Fact}{Facts}
\crefname{theorem}{Theorem}{Theorems}
\crefname{corollary}{Corollary}{Corollaries}
\crefname{claim}{Claim}{Claims}
\crefname{example}{Example}{Examples}
\crefname{algorithm}{Algorithm}{Algorithms}
\crefname{problem}{Problem}{Problems}
\crefname{definition}{Definition}{Definitions}
\usepackage{paralist}
\usepackage{turnstile}
\usepackage{mdframed}
\usepackage{tikz}

% MACROS

\newcommand{\Authornote}[2]{}
\newcommand{\Authornotecolored}[3]{}
\newcommand{\Authorcomment}[2]{}
\newcommand{\Authorfnote}[2]{}
\newcommand{\Pnote}{\Authornote{P}}
\newcommand{\Pcomment}{\Authorcomment{P}}
\newcommand{\Pfnote}{\Authorfnote{P}}
\newcommand{\Tnote}{\Authornotecolored{ForestGreen}{T}}
\newcommand{\Tcomment}{\Authorcomment{T}}
\newcommand{\Tfnote}{\Authorfnote{T}}
\newcommand{\Dnote}{\Authornote{D}}
\newcommand{\Dcomment}{\Authorcomment{D}}
\newcommand{\Dfnote}{\Authorfnote{D}}
\usepackage{boxedminipage}
% example:
% \center \noindent\begin{boxedminipage}{1.0\linewidth}}
% content
% \end{boxedminipage}
% \noindent
% {{{ parentheses }}}
% various bracket-like commands
% round parentheses
\newcommand{\paren}[1]{(#1)}
\newcommand{\Paren}[1]{\left(#1\right)}
\newcommand{\bigparen}[1]{\big(#1\big)}
\newcommand{\Bigparen}[1]{\Big(#1\Big)}
% square brackets
\newcommand{\brac}[1]{[#1]}
\newcommand{\Brac}[1]{\left[#1\right]}
\newcommand{\bigbrac}[1]{\big[#1\big]}
\newcommand{\Bigbrac}[1]{\Big[#1\Big]}
\newcommand{\Biggbrac}[1]{\Bigg[#1\Bigg]}
% absolute value
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\Abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\bigabs}[1]{\big\lvert#1\big\rvert}
\newcommand{\Bigabs}[1]{\Big\lvert#1\Big\rvert}
% cardinality
\newcommand{\card}[1]{\lvert#1\rvert}
\newcommand{\Card}[1]{\left\lvert#1\right\rvert}
\newcommand{\bigcard}[1]{\big\lvert#1\big\rvert}
\newcommand{\Bigcard}[1]{\Big\lvert#1\Big\rvert}
% set
\newcommand{\set}[1]{\{#1\}}
\newcommand{\Set}[1]{\left\{#1\right\}}
\newcommand{\bigset}[1]{\big\{#1\big\}}
\newcommand{\Bigset}[1]{\Big\{#1\Big\}}
% norm
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\Norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\bignorm}[1]{\big\lVert#1\big\rVert}
\newcommand{\Bignorm}[1]{\Big\lVert#1\Big\rVert}
\newcommand{\Biggnorm}[1]{\Bigg\lVert#1\Bigg\rVert}
% 2-norm
\newcommand{\normt}[1]{\norm{#1}_2}
\newcommand{\Normt}[1]{\Norm{#1}_2}
\newcommand{\bignormt}[1]{\bignorm{#1}_2}
\newcommand{\Bignormt}[1]{\Bignorm{#1}_2}
% 2-norm squared
\newcommand{\snormt}[1]{\norm{#1}^2_2}
\newcommand{\Snormt}[1]{\Norm{#1}^2_2}
\newcommand{\bigsnormt}[1]{\bignorm{#1}^2_2}
\newcommand{\Bigsnormt}[1]{\Bignorm{#1}^2_2}
% norm squared
\newcommand{\snorm}[1]{\norm{#1}^2}
\newcommand{\Snorm}[1]{\Norm{#1}^2}
\newcommand{\bigsnorm}[1]{\bignorm{#1}^2}
\newcommand{\Bigsnorm}[1]{\Bignorm{#1}^2}
% 1-norm
\newcommand{\normo}[1]{\norm{#1}_1}
\newcommand{\Normo}[1]{\Norm{#1}_1}
\newcommand{\bignormo}[1]{\bignorm{#1}_1}
\newcommand{\Bignormo}[1]{\Bignorm{#1}_1}
% infty-norm
\newcommand{\normi}[1]{\norm{#1}_\infty}
\newcommand{\Normi}[1]{\Norm{#1}_\infty}
\newcommand{\bignormi}[1]{\bignorm{#1}_\infty}
\newcommand{\Bignormi}[1]{\Bignorm{#1}_\infty}
% inner product
\newcommand{\iprod}[1]{\langle#1\rangle}
\newcommand{\Iprod}[1]{\left\langle#1\right\rangle}
\newcommand{\bigiprod}[1]{\big\langle#1\big\rangle}
\newcommand{\Bigiprod}[1]{\Big\langle#1\Big\rangle}
% {{{ probability }}}
% expectation, probability, variance
\newcommand{\Esymb}{\mathbb{E}}
\newcommand{\Psymb}{\mathbb{P}}
\newcommand{\Vsymb}{\mathbb{V}}
\DeclareMathOperator*{\E}{\Esymb}
\DeclareMathOperator*{\Var}{\Vsymb}
\DeclareMathOperator*{\ProbOp}{\Psymb}
\renewcommand{\Pr}{\ProbOp}
%\newcommand{\given}{\;\middle\vert\;}
\newcommand{\given}{\mathrel{}\middle\vert\mathrel{}}
%\newcommand{\given}{\mathrel{}\middle|\mathrel{}}
% {{{ miscmacros }}}
% middle delimiter in the definition of a set
\newcommand{\suchthat}{\;\middle\vert\;}
% tensor product
\newcommand{\tensor}{\otimes}
% add explanations to math displays
\newcommand{\where}{\text{where}}
\newcommand{\textparen}[1]{\text{(#1)}}
\newcommand{\using}[1]{\textparen{using #1}}
\newcommand{\smallusing}[1]{\text{(\small using #1)}}
\newcommand{\by}[1]{\textparen{by #1}}
% spectral order (Loewner order)
\newcommand{\sge}{\succeq}
\newcommand{\sle}{\preceq}
% smallest and largest eigenvalue
\newcommand{\lmin}{\lambda_{\min}}
\newcommand{\lmax}{\lambda_{\max}}
\newcommand{\signs}{\set{1,-1}}
\newcommand{\varsigns}{\set{\pm 1}}
\newcommand{\maximize}{\mathop{\textrm{maximize}}}
\newcommand{\minimize}{\mathop{\textrm{minimize}}}
\newcommand{\subjectto}{\mathop{\textrm{subject to}}}
\renewcommand{\ij}{{ij}}
% symmetric difference
\newcommand{\symdiff}{\Delta}
\newcommand{\varsymdiff}{\bigtriangleup}
% set of bits
\newcommand{\bits}{\{0,1\}}
\newcommand{\sbits}{\{\pm1\}}
% no stupid bullets for itemize environmentx
% \renewcommand{\labelitemi}{--}
% control white space of list and display environments
\newcommand{\listoptions}{\labelsep0mm\topsep-0mm\itemindent-6mm\itemsep0mm}
\newcommand{\displayoptions}[1]{\abovedisplayshortskip#1mm\belowdisplayshortskip#1mm\abovedisplayskip#1mm\belowdisplayskip#1mm}
% short for emptyset
%\newcommand{\eset}{\emptyset}
% moved to mathabbreviations
% short for epsilon
%\newcommand{\e}{\epsilon}
% moved to mathabbreviations
% super index with parentheses
\newcommand{\super}[2]{#1^{\paren{#2}}}
\newcommand{\varsuper}[2]{#1^{\scriptscriptstyle\paren{#2}}}
% tensor power notation
\newcommand{\tensorpower}[2]{#1^{\tensor #2}}
% multiplicative inverse
\newcommand{\inv}[1]{{#1^{-1}}}
% dual element
\newcommand{\dual}[1]{{#1^*}}
% subset
%\newcommand{\sse}{\subseteq}
% moved to mathabbreviations
% vertical space in math formula
\newcommand{\vbig}{\vphantom{\bigoplus}}
% setminus
\newcommand{\sm}{\setminus}
% define something by an equation (display)
\newcommand{\defeq}{\stackrel{\mathrm{def}}=}
% define something by an equation (inline)
\newcommand{\seteq}{\mathrel{\mathop:}=}
% declare function f by $f \from X \to Y$
\newcommand{\from}{\colon}
% big middle separator (for conditioning probability spaces)
\newcommand{\bigmid}{~\big|~}
\newcommand{\Bigmid}{~\Big|~}
\newcommand{\Mid}{\;\middle\vert\;}
% better vector definition and some variations
%\renewcommand{\vec}[1]{{\bm{#1}}}
\newcommand{\bvec}[1]{\bar{\vec{#1}}}
\newcommand{\pvec}[1]{\vec{#1}'}
\newcommand{\tvec}[1]{{\tilde{\vec{#1}}}}
% punctuation at the end of a displayed formula
\newcommand{\mper}{\,.}
\newcommand{\mcom}{\,,}
% inner product for matrices
\newcommand\bdot\bullet
% transpose
\newcommand{\trsp}[1]{{#1}^\dagger}
% indicator function / vector
\DeclareMathOperator{\Ind}{\mathbf 1}
% place a qed symbol inside display formula
%\qedhere
% {{{ mathoperators }}}
\DeclareMathOperator{\Inf}{Inf}
\DeclareMathOperator{\Tr}{Tr}
%\newcommand{\Tr}{\mathrm{Tr}}
\DeclareMathOperator{\SDP}{SDP}
\DeclareMathOperator{\sdp}{sdp}
\DeclareMathOperator{\val}{val}
\DeclareMathOperator{\LP}{LP}
\DeclareMathOperator{\OPT}{OPT}
\DeclareMathOperator{\opt}{opt}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\qpoly}{qpoly}
\DeclareMathOperator{\qpolylog}{qpolylog}
\DeclareMathOperator{\qqpoly}{qqpoly}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\polylog}{polylog}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\Conv}{Conv}
\DeclareMathOperator{\rank}{rank}
% operators with limits
\DeclareMathOperator*{\median}{median}
\DeclareMathOperator*{\Median}{Median}
% smaller summation/product symbols
\DeclareMathOperator*{\varsum}{{\textstyle \sum}}
\DeclareMathOperator{\tsum}{{\textstyle \sum}}
\let\varprod\undefined
\DeclareMathOperator*{\varprod}{{\textstyle \prod}}
\DeclareMathOperator{\tprod}{{\textstyle \prod}}
% {{{ textabbreviations }}}
% some abbreviations
\newcommand{\ie}{i.e.,\xspace}
\newcommand{\eg}{e.g.,\xspace}
\newcommand{\Eg}{E.g.,\xspace}
\newcommand{\phd}{Ph.\,D.\xspace}
\newcommand{\msc}{M.\,S.\xspace}
\newcommand{\bsc}{B.\,S.\xspace}
\newcommand{\etal}{et al.\xspace}
\newcommand{\iid}{i.i.d.\xspace}
% {{{ foreignwords }}}
\newcommand\naive{na\"{\i}ve\xspace}
\newcommand\Naive{Na\"{\i}ve\xspace}
\newcommand\naively{na\"{\i}vely\xspace}
\newcommand\Naively{Na\"{\i}vely\xspace}
% {{{ names }}}
% Hungarian/Polish/East European names
\newcommand{\Erdos}{Erd\H{o}s\xspace}
\newcommand{\Renyi}{R\'enyi\xspace}
\newcommand{\Lovasz}{Lov\'asz\xspace}
\newcommand{\Juhasz}{Juh\'asz\xspace}
\newcommand{\Bollobas}{Bollob\'as\xspace}
\newcommand{\Furedi}{F\"uredi\xspace}
\newcommand{\Komlos}{Koml\'os\xspace}
\newcommand{\Luczak}{\L uczak\xspace}
\newcommand{\Kucera}{Ku\v{c}era\xspace}
\newcommand{\Szemeredi}{Szemer\'edi\xspace}
\newcommand{\Hastad}{H{\aa}stad\xspace}
\newcommand{\Hoelder}{H\"{o}lder\xspace}
\newcommand{\Holder}{\Hoelder}
\newcommand{\Brandao}{Brand\~ao\xspace}
% {{{ numbersets }}}
% number sets
\newcommand{\Z}{\mathbb Z}
\newcommand{\N}{\mathbb N}
\newcommand{\R}{\mathbb R}
\newcommand{\C}{\mathbb C}
\newcommand{\Rnn}{\R_+}
\newcommand{\varR}{\Re}
\newcommand{\varRnn}{\varR_+}
\newcommand{\varvarRnn}{\R_{\ge 0}}
% {{{ problems }}}
% macros to denote computational problems
% use texorpdfstring to avoid problems with hyperref (can use problem
% macros also in headings
\newcommand{\problemmacro}[1]{\texorpdfstring{\textup{\textsc{#1}}}{#1}\xspace}
\newcommand{\pnum}[1]{{\footnotesize #1}}
% list of problems
\newcommand{\uniquegames}{\problemmacro{unique games}}
\newcommand{\maxcut}{\problemmacro{max cut}}
\newcommand{\multicut}{\problemmacro{multi cut}}
\newcommand{\vertexcover}{\problemmacro{vertex cover}}
\newcommand{\balancedseparator}{\problemmacro{balanced separator}}
\newcommand{\maxtwosat}{\problemmacro{max \pnum{3}-sat}}
\newcommand{\maxthreesat}{\problemmacro{max \pnum{3}-sat}}
\newcommand{\maxthreelin}{\problemmacro{max \pnum{3}-lin}}
\newcommand{\threesat}{\problemmacro{\pnum{3}-sat}}
\newcommand{\labelcover}{\problemmacro{label cover}}
\newcommand{\setcover}{\problemmacro{set cover}}
\newcommand{\maxksat}{\problemmacro{max $k$-sat}}
\newcommand{\mas}{\problemmacro{maximum acyclic subgraph}}
\newcommand{\kwaycut}{\problemmacro{$k$-way cut}}
\newcommand{\sparsestcut}{\problemmacro{sparsest cut}}
\newcommand{\betweenness}{\problemmacro{betweenness}}
\newcommand{\uniformsparsestcut}{\problemmacro{uniform sparsest cut}}
\newcommand{\grothendieckproblem}{\problemmacro{Grothendieck problem}}
\newcommand{\maxfoursat}{\problemmacro{max \pnum{4}-sat}}
\newcommand{\maxkcsp}{\problemmacro{max $k$-csp}}
\newcommand{\maxdicut}{\problemmacro{max dicut}}
\newcommand{\maxcutgain}{\problemmacro{max cut gain}}
\newcommand{\smallsetexpansion}{\problemmacro{small-set expansion}}
\newcommand{\minbisection}{\problemmacro{min bisection}}
\newcommand{\minimumlineararrangement}{\problemmacro{minimum linear arrangement}}
\newcommand{\maxtwolin}{\problemmacro{max \pnum{2}-lin}}
\newcommand{\gammamaxlin}{\problemmacro{$\Gamma$-max \pnum{2}-lin}}
\newcommand{\basicsdp}{\problemmacro{basic sdp}}
\newcommand{\dgames}{\problemmacro{$d$-to-1 games}}
\newcommand{\maxclique}{\problemmacro{max clique}}
\newcommand{\densestksubgraph}{\problemmacro{densest $k$-subgraph}}
% {{{ alphabet }}}
\newcommand{\cA}{\mathcal A}
\newcommand{\cB}{\mathcal B}
\newcommand{\cC}{\mathcal C}
\newcommand{\cD}{\mathcal D}
\newcommand{\cE}{\mathcal E}
\newcommand{\cF}{\mathcal F}
\newcommand{\cG}{\mathcal G}
\newcommand{\cH}{\mathcal H}
\newcommand{\cI}{\mathcal I}
\newcommand{\cJ}{\mathcal J}
\newcommand{\cK}{\mathcal K}
\newcommand{\cL}{\mathcal L}
\newcommand{\cM}{\mathcal M}
\newcommand{\cN}{\mathcal N}
\newcommand{\cO}{\mathcal O}
\newcommand{\cP}{\mathcal P}
\newcommand{\cQ}{\mathcal Q}
\newcommand{\cR}{\mathcal R}
\newcommand{\cS}{\mathcal S}
\newcommand{\cT}{\mathcal T}
\newcommand{\cU}{\mathcal U}
\newcommand{\cV}{\mathcal V}
\newcommand{\cW}{\mathcal W}
\newcommand{\cX}{\mathcal X}
\newcommand{\cY}{\mathcal Y}
\newcommand{\cZ}{\mathcal Z}
\newcommand{\scrA}{\mathscr A}
\newcommand{\scrB}{\mathscr B}
\newcommand{\scrC}{\mathscr C}
\newcommand{\scrD}{\mathscr D}
\newcommand{\scrE}{\mathscr E}
\newcommand{\scrF}{\mathscr F}
\newcommand{\scrG}{\mathscr G}
\newcommand{\scrH}{\mathscr H}
\newcommand{\scrI}{\mathscr I}
\newcommand{\scrJ}{\mathscr J}
\newcommand{\scrK}{\mathscr K}
\newcommand{\scrL}{\mathscr L}
\newcommand{\scrM}{\mathscr M}
\newcommand{\scrN}{\mathscr N}
\newcommand{\scrO}{\mathscr O}
\newcommand{\scrP}{\mathscr P}
\newcommand{\scrQ}{\mathscr Q}
\newcommand{\scrR}{\mathscr R}
\newcommand{\scrS}{\mathscr S}
\newcommand{\scrT}{\mathscr T}
\newcommand{\scrU}{\mathscr U}
\newcommand{\scrV}{\mathscr V}
\newcommand{\scrW}{\mathscr W}
\newcommand{\scrX}{\mathscr X}
\newcommand{\scrY}{\mathscr Y}
\newcommand{\scrZ}{\mathscr Z}
\newcommand{\bbB}{\mathbb B}
\newcommand{\bbS}{\mathbb S}
\newcommand{\bbR}{\mathbb R}
\newcommand{\bbZ}{\mathbb Z}
\newcommand{\bbI}{\mathbb I}
\newcommand{\bbQ}{\mathbb Q}
\newcommand{\bbP}{\mathbb P}
\newcommand{\bbE}{\mathbb E}
\newcommand{\bbN}{\mathbb N}
\newcommand{\sfE}{\mathsf E}
% {{{ leqslant }}}
% slanted lower/greater equal signs
\renewcommand{\leq}{\leqslant}
\renewcommand{\le}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\ge}{\geqslant}
% {{{ varepsilon }}}
\let\epsilon=\varepsilon
% {{{ numberequationwithinsection }}}
\numberwithin{equation}{section}
% {{{ restate }}}
% set of macros to deal with restating theorem environments (or anything
% else with a label)
% adapted from Boaz Barak
\newcommand\MYcurrentlabel{xxx}
% \MYstore{A}{B} assigns variable A value B
\newcommand{\MYstore}[2]{%
  \global\expandafter \def \csname MYMEMORY #1 \endcsname{#2}%
}
% \MYload{A} outputs value stored for variable A
\newcommand{\MYload}[1]{%
  \csname MYMEMORY #1 \endcsname%
}
% new label command, stores current label in \MYcurrentlabel
\newcommand{\MYnewlabel}[1]{%
  \renewcommand\MYcurrentlabel{#1}%
  \MYoldlabel{#1}%
}
% new label command that doesn't do anything
\newcommand{\MYdummylabel}[1]{}
\newcommand{\torestate}[1]{%
  % overwrite label command
  \let\MYoldlabel\label%
  \let\label\MYnewlabel%
  #1%
  \MYstore{\MYcurrentlabel}{#1}%
  % restore old label command
  \let\label\MYoldlabel%
}
\newcommand{\restatetheorem}[1]{%
  % overwrite label command with dummy
  \let\MYoldlabel\label
  \let\label\MYdummylabel
  \begin{theorem*}[Restatement of \cref{#1}]
    \MYload{#1}
  \end{theorem*}
  \let\label\MYoldlabel
}
\newcommand{\restatelemma}[1]{%
  % overwrite label command with dummy
  \let\MYoldlabel\label
  \let\label\MYdummylabel
  \begin{lemma*}[Restatement of \cref{#1}]
    \MYload{#1}
  \end{lemma*}
  \let\label\MYoldlabel
}
\newcommand{\restateprop}[1]{%
  % overwrite label command with dummy
  \let\MYoldlabel\label
  \let\label\MYdummylabel
  \begin{proposition*}[Restatement of \cref{#1}]
    \MYload{#1}
  \end{proposition*}
  \let\label\MYoldlabel
}
\newcommand{\restatefact}[1]{%
  % overwrite label command with dummy
  \let\MYoldlabel\label
  \let\label\MYdummylabel
  \begin{fact*}[Restatement of \prettyref{#1}]
    \MYload{#1}
  \end{fact*}
  \let\label\MYoldlabel
}
\newcommand{\restate}[1]{%
  % overwrite label command with dummy
  \let\MYoldlabel\label
  \let\label\MYdummylabel
  \MYload{#1}
  \let\label\MYoldlabel
}
% {{{ mathabbreviations }}}
\newcommand{\la}{\leftarrow}
\newcommand{\sse}{\subseteq}
\newcommand{\ra}{\rightarrow}
\newcommand{\e}{\epsilon}
\newcommand{\eps}{\epsilon}
\newcommand{\eset}{\emptyset}
% {{{ allowdisplaybreaks }}}
% allows page breaks in large display math formulas
\allowdisplaybreaks
% {{{ sloppy }}}
% avoid math spilling on margin
\sloppy
\newcommand*{\bT}{\mathbf{T}}
\newcommand*{\bY}{\mathbf{Y}}
\newcommand*{\bX}{\mathbf{X}}
\newcommand*{\xdist}{\sigma}
\newcommand*{\jdist}{\cJ}
\newcommand*{\jndist}{\cJ_{\emptyset}}
\newcommand*{\jfdist}{\cJ_{*}}
\newcommand*{\plantedclique}{planted clique}
\newcommand*{\tensordecomposition}{tensor decomposition}
\newcommand*{\subdist}{\Upsilon}
\newcommand*{\planteddist}{\cD_{*}}
\newcommand*{\lplus}{\lambda_{+}}
\newcommand*{\instances}{\cP}
\newcommand*{\One}{1}
\newcommand*{\randomdist}{\cD_{\emptyset}}
\newcommand*{\prior}{\pi}
\newcommand*{\gnhalf}{\mathbb{G}\left(n,\frac{1}{2}\right)}
\newcommand*{\nulld}{\cD_{\emptyset}}
\newcommand*{\feasd}{\cD_{*}}
\newcommand*{\Id}{\mathrm{Id}}
\newcommand*{\sphere}{\mathbb{S}^{n-1}}
\newcommand*{\tr}{\mathrm{tr}}
\newcommand*{\Normop}[1]{\Norm{#1}}
\newcommand*{\Lowner}{L\"owner\xspace}
\newcommand*{\normtv}[1]{\Norm{#1}_{\mathrm{TV}}}
\newcommand*{\normf}[1]{\Norm{#1}_{\mathrm{F}}}
\newcommand*{\pclique}{\problemmacro{planted clique}}
\newcommand*{\kclique}{\problemmacro{$k$-clique}}
\newcommand*{\injtnorm}{\problemmacro{injective tensor norm}}
\newcommand*{\tensorpca}{\problemmacro{tensor PCA}}
\newcommand*{\threexor}{\problemmacro{3-xor}}
\newcommand*{\fourxor}{\problemmacro{4-xor}}
\newcommand*{\F}{\mathbb{F}}
\newcommand*{\tmu}{\tilde{\mu}}
\newcommand*{\norminj}[1]{\norm{#1}_{\mathrm{inj}}}
\newcommand*{\pdistset}[2]{\{\tilde{\cD}\}_{#1,#2}}
\newcommand*{\normop}[1]{\norm{#1}_{\mathrm{op}}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator*{\pE}{\tilde{\mathbb{E}}}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator*{\argmin}{argmin}
% \newcommand*{\tran}{^{\mkern-1.5mu\mathsf{T}}}
\newcommand*{\transpose}[1]{{#1}{}^{\mkern-1.5mu\mathsf{T}}}
\newcommand*{\dyad}[1]{#1#1{}^{\mkern-1.5mu\mathsf{T}}}

% TITLE

\title{
Most Informative Bit}

% AUTHOR

\author{}

\begin{document}

\pagestyle{empty}

% MAKE TITLE


\maketitle
\thispagestyle{empty} % seems to be required here to avoid page number on first page


% ABSTRACT


\clearpage

% TOC


  % assumes microtype
 % \microtypesetup{protrusion=false}
 % \tableofcontents{}
 % \microtypesetup{protrusion=true}


\clearpage

\pagestyle{plain}
\setcounter{page}{1}

% SECTION



  % assumes hyperref
%  \phantomsection
 % \addcontentsline{toc}{section}{References}
 % \bibliographystyle{amsalpha}
 % \bibliography{bib/mathreview,bib/dblp,bib/custom,bib/scholar}


\section{Continous Version}

Let $f: [0,1]^n \to \R$ be the multilinear extension of $F$.  Let 
\[ s_i(x) \defeq \begin{cases} \sqrt{x_i \cdot (1-x_i)} \cdot r_i(x)  & \text{ if } x_i \in (\alpha,1-\alpha) \\
 0 & \text{otherwise} \end{cases} \]

%For $X \in [0,1]^n$, let $s(X) = (s(X_1),\ldots,s(X_n))$.

Define the stochastic process $X \in \R^n$ by
\[ dX_i  = s_i(X) dB_i\]
for $i \in \{1,\ldots, n\}$.   We are using $X$ as short for $X(t)$ and $dB$ is the standard Brownian motion.  We start the stochastic process at $X(0) = (\frac{1}{2},\frac{1}{2},\ldots,\frac{1}{2})$.  Each coordinate $X_\iota$ does a random walk until it touches either $\alpha$ or $1-\alpha$ and it stops.

Let $h(p) = -p\log p - (1-p) \log (1-p)$ be the Shannon entropy function.


%By Ito's lemma, we can write the entropy of a bit with bias $X_i$
%\begin{align*}
%dh(X_i) & = h'(X_i) dX_i + \frac{h''(X_i)}{2} (dX_i)^2 \\
%	& = h'(X_i) dX_i - \frac{1}{X_i(1-X_i)} (\sqrt{X_i(1-X_i)})^2 dt \\
%& = h'(X_i) dX_i -  \frac{1}{2} dt
%\end{align*}
%when $X_i \in (\alpha,1-\alpha)$.  But $dh(X_i) = 0$ once $X_i \notin (\alpha,  1-\alpha)$.
%
%\begin{align}\label{eq1}
%E[h(X_i(t))] = 1- \frac{1}{2} \int 1[X_i \in (\alpha,1-\alpha)] dt.  
%\end{align}

%The expected entropy of $X_i$ drops linearly with time.


Now we will calculate $E[h(f(X(t)))]$.  First, let us compute $df(X)$ as follows, 
\begin{align}
df(X) & = \langle \nabla f(X), dX \rangle + \left( \sum_{i,j} \frac{\partial^2 f}{\partial X_i \partial X_j} dX_i dX_j\right) \\
	  & = \langle \nabla f(X), s(X)dB \rangle = \sum_{i = 1}^n \partial_i f(X) s_i(X) dB_i
\end{align}
where the $\frac{\partial^2 f}{\partial X_i^2}$ vanish since $f$ is multilinear and $dB_i dB_j = 0$ for $i \neq j$.


\begin{enumerate}
\item  As long as $\norm{r(x)} > 0$ whenever $x \in (\alpha,1-\alpha)^n$, $\norm{X}_2^2$ is a submartingale, so the process only stops on reaching $\{\alpha,1-\alpha\}^n$

\item For every choice of function $r_1(x),r_2(x),\ldots, r_n(x)$,  for every function $F:\bits^n \to \bits$ the above calculation shows that $f$ is a martingale.  Hence $\E[f(x)] = f(\frac{1}{2},\ldots, \frac{1}{2})$.  As this holds for every function $F: \bits^n \to \bits$, we conclude $X(t)$ is distributed uniformly over $\{\alpha,1-\alpha\}^n$ in the limit $t \to \infty$.

\end{enumerate}

Now, we are ready to calculate $dh(f(X))$,
\begin{align*}
dh(f(X)) & = h'(f(X)) df(X) + \frac{1}{2} h''(f(X)) \cdot (df(X))^2 \\
         & = h'(f(X)) df(X) + \frac{1}{2} h''(f(X)) \langle \nabla f(X),s(X) dB \rangle^2 \\
         & = h'(f(X)) \langle \nabla f(X), s(X)dB \rangle + \frac{1}{2} h''(f(X)) \left(\sum_i (\partial_i f(X))^2 s_i^2(X)\right) dt\\
         & = h'(f(X)) \langle \nabla f(X), s(X)dB \rangle - \frac{1}{2} \frac{1}{f(X)(1-f(X))} \left(\sum_i (\partial_i f(X))^2 s_i^2(X)\right) dt\\
\end{align*}

Therefore we can write 
\begin{align} \label{eq2}
\E[h(f(X[t]))] = h(E[F]) - \E \frac{1}{2} \int_0^t \frac{\sum_i (\partial_i f(X))^2 s_i^2(X_i) } {f(X)(1-f(X))} dt  
\end{align}


Let $\mu_t$ denote the product distribution over $\{0,1\}^n$ with marginals $X(t)$.  Then notice that the variance $\Var_{X \sim \mu_t} [F(x)] = f(x)(1-f(x))$.  Furthermore, by writing down the Efron-Stein basis $\chi_i(0) = -\sqrt{\frac{x}{(1-x)}}$ and $\chi_i(1) = \sqrt{\frac{(1-x)}{x}}$, one can calculate the coefficients $\hat{F}_i$ and they satisfy,
\[ \partial_i f(X) = \hat{F}_i \frac{1}{\sqrt{X_i(1-X_i)}}\]
and 
\[f(X)(1-f(X)) = \Var_{\mu_t}[F] = \sum_S \hat{F}_S^2 \]
%Let $\mu_t$ denote the product distribution over $\{0,1\}^n$ with marginals $X(t)$.  Then notice that the variance $\Var_{X \sim \mu_t} [F(x)] = f(x)(1-f(x))$.  Furthermore, $\sum_i (\partial_i f(X))^2 s^2(X_i) = \| F^{=1}(x)\|^2$ is the norm of the degree $1$ part of the function $F$ under Efron-Stein decomposition under $\mu_t$.
Substituting the above facts in \eqref{eq2}, we get
\begin{align} \label{eq34}
 h(E[F]) - \E[h(f(X[t]))]  =  \E \frac{1}{2} \int_0^t \sum_i \frac{\hat{F}_i^2}{\Var_{\mu_t}[F]} \cdot r_i(X)^2 \cdot 1[X_i \in (\alpha,1-\alpha)]  dt  
\end{align}

Substitute $F(x) = x_i$ in the above equation, and set $t \to \infty$ we get,
\begin{align} \label{eq35}
1 - h(\alpha) =  \E \frac{1}{2} \int_{0}^{\infty}  r_i(X)^2 \cdot 1[X_i \in (\alpha,1-\alpha)]  dt  
\end{align}
for every $i$

Our goal is to show that for every boolean function $F$ for sufficiently large $t$.
\begin{align} \label{eq36}
h(E[F]) - \E[h(f(X(t)))] \leq 1 - h(\alpha)
\end{align}





\subsection{Charging Argument}

Split every sample path into $n$ segments $\cP = \cP_0 \cup \cP_1 \cup \ldots \cup \cP_{n-1}$ where segment $\cP_i$ has $n-i$ active variables.
Let $Z_i$ denote the starting point of segment $\cP_i$ and let $Z_n$ be ending point of $\cP_n$.  Let $\tau_i$ denote the time at beginning of segment $\cP_i$.  $\tau_i$ is a stopping time.


For each segment $\cP_i$, we can write

\begin{align}
	h(Z_{i+1})-h(Z_i) = \int_{\tau_i}^{\tau_{i+1}} h'(f(X))\iprod{f(X), s(X)dB} - \frac{1}{2} \int_{\tau_i}^{\tau_{i+1}} \sum_{i} \frac{\hat{F}_i^2s_i(X)^2}{\Var_{\mu_t}[F]}
\end{align}

For each conditioning of $\tau_i$ and $Z_i$,  $\tau_{i+1}$ is a stopping time.  By optimal stopping theorem applied to the Martingale term,
\begin{align}
\E\left[ \int_{\tau_i}^{\tau_{i+1}} h'(f(X))\iprod{f(X), s(X)dB} | Z_i,\tau_i     \right] = 0
\end{align}
for each $\tau_i$ and $Z_i$.

Therefore we can write,
\begin{align}
	\E[h(Z_{i+1})]- \E[h(Z_i)] =  - \E \frac{1}{2} \int_{\tau_i}^{\tau_{i+1}} \sum_{i} \frac{\hat{F}_i^2s_i(X)^2}{\Var_{\mu_t}[F]}
\end{align}



Can we choose $\{ r_i(x)\}_{i\in [n]}$ depending on $f$ such that we can use  \eqref{eq34} and \eqref{eq35} to show $\eqref{eq36}$?   

We need to understand $\hat{F}_i$ better.  If $\hat{F}_i$ behaved arbitrarily, then it could be that $\hat{F}_1^2/\Var[F] = 1$ until coordinate $1$ is active, and then $\hat{F}_2^2 /\Var[F] = 1$ until coordinate $2$ remains active.  Then RHS of \eqref{eq34} would be stricly larger than \eqref{eq35}.  This should not happen.


%Can we do this without any assumption on how $\hat{F}_i^2$ behave?  If not, what properties of $\hat{F}_i$ would we need?


\iffalse
Let $\{\sigma^0, \sigma^1,\ldots,\sigma^{n-1}\}$ denote cyclic permutations of $\{1,\ldots,n\}$.  For a function $F : \bits^n \to \bits$, define $F^{(i)}(x) = F(\sigma^i \circ x)$.   We will now write \eqref{eq34} for each function $F^{(i)}$ (and its corresponding multilinear extension $f^{(i)}$) and sum.  Since the distribution of $X[t]$ is invariant under permutation of coordinates, the LHS remains invariant under these cyclic permutations.  So we get:
\begin{align}
n \cdot \E[h(f(X[t]))] & = n \cdot h(E[F]) - \E \frac{1}{2} \int_0^t \sum_i \sum_j \frac{\hat{F}_{\sigma^{j}(i)}^2}{\Var_{\mu_t}[F]} \cdot 1[X_i \in (\alpha,1-\alpha)]  dt \\
& =  n \cdot h(E[F]) -  \E \frac{1}{2} \int_0^t  \left(\sum_j \frac{\hat{F}_{j}^2}{\Var_{\mu_t}[F]} \right) \cdot \left(\sum_i 1[X_i \in (\alpha,1-\alpha)] \right) dt
\end{align}
Hence we can conclude the following for every function $F:\bits^n \to \bits$ for all time $t$,
\begin{align}
h(E[F]) - \E[h(f(X[t]))] = \frac{1}{2n} \int_0^t  \left(\sum_j \frac{\hat{F}_{j}^2}{\Var_{\mu_t}[F]} \right) \cdot \left(\sum_i 1[X_i \in (\alpha,1-\alpha)] \right) dt
\end{align}
Notice that 
\[ \sum_j \frac{\hat{F}_{j}^2}{\Var_{\mu_t}[F]} \leq 1 \]
for all functions since $\Var_{\mu_t} [F] = \sum_{S} \hat{F}_S^2$ for sets $S$
 of all sizes.  Equality holds only if $F$ is linear (like a dictator).  Hence, we conclude the following theorem:

 \begin{theorem}
	For all time $t$, among all boolean functions $F:\{0,1\}^n \to \{0,1\}$, the dictators maximize the quantity,
	\[ h(E[F]) - \E[h(f(X))]\] where $X$ is the stochastic process $dX = s(X)dB$.
 \end{theorem}

To recover the Courtade-Kumar conjecture, we check that as $t \to \infty$, $X(t) \in \{\alpha,1-\alpha\}^n$ almost surely.  Moreover, by symmetry of the stochastic process $dX = s(X) dB$, distribution of $X(t)$ will be uniform over $\{\alpha,1-\alpha\}^n$.
\fi


\iffalse
\section{General Continous Version}

Let $f: [0,1]^n \to \R$ be the multilinear extension of $F$.  

Let $S: [0,1]^n \to \R^{n \times n}$ be a reasonably nice function.  Define the stochastic process $X \in \R^n$ by
\[ dX  = S(X) dB\]
where $dB = (dB_1,\ldots,dB_n)$ is the usual Brownian motion.  We start the stochastic process at $X(0) = (\frac{1}{2},\frac{1}{2},\ldots,\frac{1}{2})$


We will require $S$ to have the following properties:  
\begin{enumerate}
\item for each $i \in [n]$, the $i^{th}$ row, $S_i = (S_{ij})_{j \in [n]}$ satisfies $S_{i}(x) = 0$ whenever $x_i \in (1-\alpha,1) \cup (0,\alpha)$.
%
This decay property is to ensures that any coordinate stops changing when it either reaches $\alpha$ or $1-\alpha$, and that our random walks never touch $\{0,1\}$ along any coordinate.

\item We will require that $S(X) \neq 0$ whenever there is a coordinate $X_i \notin \{\alpha, 1-\alpha\}$.  Under this condition, we will have,
\[ d\norm{X}^2 = \Tr(S(X) S(X)^T) dt\]
, thus the stochastic process keeps increasing $\norm{X}$ and doesn't stop until it is $X \in \{\alpha,1-\alpha\}^n$.

\item (DIAGONAL)  We will assume $S$ is diagonal matrix.
\end{enumerate}

For any function $F: \{0,1\}^n \to \{0,1\}$, consider the multilinear extension of $f$.
Since the second derivatives of $f$ are zero,
\begin{align}
	df(X) = (\nabla f)^T S dB 
\end{align}
Hence $f(X)$ is a martingale, and we have for all time $t$, $\E[f(X(t))] = f(X(0)) = \E_x [F(x)]$.  Since this equality holds for all functions $F$, as $t \to \infty$, $X(t)$ is distributed uniformly over $\{\alpha,1-\alpha\}^n$ for every choice of function $S$ satisfying property $(1)$ and $(2)$.  



If $h(p) = p \log p + (1-p) \log (1-p)$ then
\begin{align} \label{eq3}
	d h(f(X)) = h'(f(X)) (\nabla f)^T S dB - \frac{1}{2} \frac{1}{f(X)(1-f(X))} \cdot (\nabla f)^T S S^T (\nabla f)
\end{align}

Let $\mu_t$ denote the product distribution over $\{0,1\}^n$ with marginals $X(t)$.  Then notice that the variance $\Var_{X \sim \mu_t} [F(x)] = f(x)(1-f(x))$.  Furthermore, by writing down the Efron-Stein basis $\chi_i(0) = -\sqrt{\frac{x}{(1-x)}}$ and $\chi_i(1) = \sqrt{\frac{(1-x)}{x}}$, one can calculate the coefficients $\hat{F}_i$ and they satisfy,
\[ \partial_i f(X) = \hat{F}_i \frac{1}{\sqrt{X_i(1-X_i)}}\]
and 
\[f(X)(1-f(X)) = \Var_{\mu_t}[F] = \sum_S \hat{F}_S^2 \]

Define a diagonal matrix $D$ with $D_{ii} \defeq \frac{1}{\sqrt{x_i(1-x_i)}}$ and let  $\tilde{S} = SD$.
Define the vector $v(X) \in \R^n$ by $v_i(X) \defeq \frac{\hat{F}_i}{\sqrt{f(X)(1-f(X))}}$.  

We can rewrite \eqref{eq3} as,
\begin{align} \label{eq4}
	d h(f(X)) = h'(f(X)) (\nabla f)^T S dB - \frac{1}{2} (v(X)^T \tilde{S} \tilde{S}^T (v(X))
\end{align}
In expectation, we get 
\begin{align} \label{eq5}
	\E[h(f(X))] = h(E[F]) - \frac{1}{2} \E \left[ \int_0^t  (v(X)^T \tilde{S} \tilde{S}^T (v(X)) dt\right]
\end{align}
If $F(x) = x_i$ is a dictator then for all $X$, $v(X) = e_i$ the $i^{th}$ basis vector $e_i$.  So we get,
\begin{align}
	\E[h(X_i)] = 1 - \frac{1}{2} \E \left[ \int_0^t  (\tilde{S}\tilde{S}^T)_{ii} dt \right]
\end{align}
For all functions $F$, $\norm{v(X)} \leq 1$ for all $X$.


\noindent \textbf{Idea:}  For every $x \in [\alpha,1-\alpha]^n$, let active coordinates of $x$ be $A_x \defeq \{j | x_j \notin \{\alpha,1-\alpha\}\}$.  Construct an $S$ depending on $F$ such that for all $x \in [\alpha,1-\alpha]^n$, and every active coordinate $i \in A_x$,
 \[ (\tilde{S}\tilde{S}^T)_{ii} \geq v(X)^T \tilde{S} \tilde{S}^T (v(X)) \]

If the above "dictator domination" condition holds then for each path, the value is dominated by the dictator that remains active till the very end.

We will now try to construct a diagonal matrix $S$ with properties $(1)$, $(2)$ and dictator dominance.

Set 
\begin{align*}
S_{ii} = \begin{cases} \sqrt{x_i(1-x_i)} & \text{ if $i$ is active} \\
			0 & \text{ otherwise }
			\end{cases}
\end{align*}
Then for each active $i$,
\begin{align*}
(\tilde{S} \tilde{S}^T)_{ii} = 1
\end{align*}
while 
\begin{align} \label{eq8}
v(X)^T (\tilde{S} \tilde{S}^T) v(X) = \sum_{i \in A_x} v_i(X)^2 \leq 1
\end{align}
since $\norm{v(X)}^2 = \frac{\hat{f}_i^2}{\Var[f]}  \leq 1$.

The matrix $S$ has a discontinuity as $x_i$ approaches $\{\alpha,1-\alpha\}$.   


\subsection{Charging argument}

Now we will try to formally show that dictator dominant $\tilde{S}$ would imply the result we want.

We can write from \eqref{eq5} and \eqref{eq8},
\begin{align} 
\E[h(f(X))] & = h(E[F]) - \frac{1}{2} \E \left[ \int_0^t  (v(X)^T \tilde{S} \tilde{S}^T (v(X)) dt\right] \nonumber \\
			& = h(E[F]) - \frac{1}{2} \E \left[ \int_0^t  \sum_{i \in A_x} v_i(X)^2 dt\right] \nonumber 
\end{align}

Let $\{\sigma^0, \sigma^1,\ldots,\sigma^{n-1}\}$ denote cyclic permutations of $\{1,\ldots,n\}$.  For a function $F : \bits^n \to \bits$, define $F^{(i)}(x) = F(\sigma^i \circ x)$.   Writing the above equation for each function $F^{(i)}$ (and its corresponding multilinear extension $f^{(i)}$) and summing up,
\begin{align}
\sum_i \E[h(f^{(i)}(x))] & = n h(E[F]) -  \E \left[ \int_0^t  \sum_{i \in A_x} \sum_{j} v_{\sigma^j(i)}(X)^2 dt\right] \nonumber\\
						 & = n h(E[F]) -  \E \left[ \int_0^t |A_x| \norm{v(X)}^2 dt \right] \nonumber\\
						 & \geq n h(E[F]) - \E \left[ \int_0^t |A_x| dt \right] \label{eq24}
\end{align}

Applying the above calculation for $F(x) = x_1$, we get 

\begin{align}\label{eq22}
\sum_i \E[h(X_i)] & = n \cdot 1 - \E \left[ \int_0^t |A_x| dt \right] 
\end{align}
As time $t \to \infty$, all coordinates of $X_i(t) \in \{\alpha, 1-\alpha\}$.  Hence, the LHS above is equal to $n h(\alpha)$.

From \eqref{eq22} and \eqref{eq24}, we get
\begin{align*}
n \E[h(f(X))] \geq n h(E[F]) - (n - n h(\alpha))
\end{align*}
which gives,
\[ 1-h(\alpha) \geq h(E[F]) - \E[h(f(X))]] \]
as desired.
\fi

\iffalse
At any given time $t$, partition the set of all paths $\cP = \cup_{i = 1}^n \cP_i$ where $\cP_i$ is set of all paths such that $i$ is the lexicographically first active coordinate at the end, i.e., at time $t$, $x_i \notin \{\alpha, 1-\alpha\}$.

(Doing it slowly, since I am on shaky ground)
We can write from \eqref{eq5},
\begin{align} 
\E[h(f(X))] & = h(E[F]) - \frac{1}{2} \E \left[ \int_0^t  (v(X)^T \tilde{S} \tilde{S}^T (v(X)) dt\right] \nonumber \\
			& = h(E[F]) - \frac{1}{2} \sum_i \Pr[\cP_i] \cdot \E \left[ \int_0^t  (v(X)^T \tilde{S} \tilde{S}^T (v(X)) dt | \cP_i \right] \nonumber\\  
			& \geq  h(E[F]) - \frac{1}{2} \sum_i \Pr[\cP_i] \cdot \E \left[ \int_0^t  (\tilde{S} \tilde{S}^T)_{ii} dt | \cP_i \right]   \label{eq7}
\end{align}
where in the last step we used dictator dominance on coordinate $i$ which was active throughout the path.

Now consider the $i$th dictator $x_i$.  We can write,
\begin{align*} 
\E[h(x_i) | \cP_i] & = 1 + \E\left[ \int h'(x_i) (\nabla x_i)^T S dB | \cP_i\right]  - \frac{1}{2} \E \left[ \int_0^t  ( \tilde{S} \tilde{S}^T)_{ii}  dt\right]
\end{align*}
%
Note that even after conditioning on $\cP_i$, the $i$th coordinate given by $dX_i = (e_i)^T S dB$ is a martingale.  So the first term on RHS is equal to zero.  
Secondly, after sufficiently long time, $x_i = \alpha $ or $x_i = 1-\alpha$ along all paths.  Moreover, $h(\alpha) = h(1-\alpha)$.   So the LHS above is equal to $h(\alpha)$.  
Therefore one can immediately conclude that for each $i$, after sufficiently long time,
\begin{align*}
 \frac{1}{2} \E \left[ \int_0^t  ( \tilde{S} \tilde{S}^T)_{ii} dt\right] = 1- h(\alpha)
\end{align*}
Substituting the above in \eqref{eq7}, we get that
\begin{align}
\E[h(f(X))] & \geq  h(E[F]) - (1-h(\alpha))
\end{align}
\fi

\iffalse 
Here the rough idea is as follows:  If $v(x)$ has two non-zero active coordinates, then we can set $S(x)$ such that $(S(x))^T v(x) = 0$ but for every active coordinate $i$, $S_i(x)=0$.   Basically, the idea is that if $v(x)$ has two non-zero coordinates, then there exists matrix $S(x)$ such that $S(x) e_i \neq 0$ for all active coordinates $i$, but $S(x) v(x) = 0$.

%Without loss of generality, let us assume that function $F$ depends on all its coordinates, otherwise, we can carry out our analysis in lower dimensional cube.  Since $F$ depends on all its coordinates, but for a set of measure zero, we will have $\hat{F}_i \neq 0$, i.e.,all coordinates of $v(x)$ are non-zero for all $x$ but for a set of measure zero (this needs more care, more on this case at the end!).

Recall $A_x$ set of active coordinates of $x$.  Let $NZ_x$ denote set of non-zero coordinates of $v(x)$.  Let $B_x = NZ_x \cap A_x$.

Let $\Pi_{A_x} \in \R^{n \times n} $ denote the projection matrix on to active coordinates, i.e., $\Pi_{A_x}$ is a diagonal matrix with $1$s for active rows and $0$ for non-active rows.

If $|B_x| > 1$ then set $S(x) = (\Pi_{A_x} - \hat{v}(x) \hat{v}(x)^T) \Pi_{A_x}$ where $\hat{v}(x) = \Pi_{A_x} \cdot D v(x)/\norm{\Pi_{A_x} D v(x)}$.  
$S(x)$ is essentially a projection matrix on to subspace orthogonal to $Dv(X)$ within the active coordinates.  Clearly, for every  $\tilde{S}\tilde{S}^T_{ii} \geq 0$, so dictator  dominance is trivially satisfied.  By construction, we have $(\tilde{S}(x))^T v(x) = SD v(x) = 0$ and $S(x)$ has zero rows on non-active coordinates.  


Finally, suppose $|B_x| = 1$.  There are two cases:  

Case 1:  $|A_x| > 1$.
	In this case, there is a coordinate $i \in A_x$ such that $v_i(x) = 0$.  Set $S = e_i e_i^T$, then $v(x)^T  (\tilde{S}\tilde{S}^T) v(x) = 0$, while $(\tilde{S}\tilde{S}^T)_{ii} > 0$.


Case 2:  $|A_x| = 1$.
 In this case, fix $S_x = e_i e_i^T$.
\begin{align*}
 v(x)^T  (\tilde{S}\tilde{S}^T) v(x) = v_i(x)^2 \frac{(1-2x_i)^2}{x_i(1-x_i)} \leq \frac{(1-2x_i)^2}{x_i(1-x_i)}  = (\tilde{S}\tilde{S}^T)_{ii} 
 \end{align*}
where we used the fact that $\norm{v(x)}_2^2 \leq 1$.  

\fi

%By construction, for every other active coordinate $j \in A_x$ 
%\[(\tilde{S}\tilde{S}^T)_{jj} = \frac{1}{x_i(1-x_i)} \cdot \frac{1}{(1-2x_j)^2} \cdot \frac{(1-2x_j)^2}{x_j(1-x_j)} \geq \frac{1}{x_i(1-x_i)} \geq \frac{(1-2x_i)^2}{x_i(1-x_i)} \geq  v(x)^T  (\tilde{S}\tilde{S}^T) v(x) \]
%The annoying case is when $v(X)$ has only one non-zero coordinate, but there are many active coordinates.  

\iffalse
\newpage


\newpage  
\section{OldStuff: Discrete Bit Revealing Process}  

Let $X \sim \{0,1\}^n$ be uniformly distributed.

Fix small $\epsilon$ (we will make $\epsilon \to 0$).

Suppose we have a bit $b$.  We will reveal the bit $b$ via a sequence of bits $z_1,\ldots,z_
\ell$ where each $z_i$ is only $\epsilon$-correlated with $b$.  Formally, inductively define the sequence of bits $z_i$ as follows:

\begin{align}
    z_1 = \begin{cases}
            b & \text{ with prob. } \epsilon \\
            \text{random bit} & \text{ with prob. } 1-\epsilon 
        \end{cases} \\
        \end{align}
        \begin{align}
    z_t = \begin{cases}
        b & \text{ with prob. } \epsilon\\ 
        \text{ random bit from distribution } b | z_1,\ldots, z_{t-1} & \text{with prob.} 1-\epsilon 
        \end{cases}
\end{align}

Apply the random process independently on each bit of $X \sim \{0,1\}^n$ to generate a sequence  $Z_1,\ldots, Z_N \subset \{0,1\}^n$.

Here are some observations:

\begin{enumerate}
	\item For each $i \in [T]$, $Z_i$ and $X|Z_1,\ldots,Z_{i-1}$ are both distributed according to the same product distribution.  Define, 
		$$  \mu_i \defeq \E[ X | Z_1,\ldots, Z_{i-1}]$$
		as the biases of these product measure (the biases $\mu_i$ is a random variable).

	\item If $T = \Omega(1/\epsilon^2)$, then each coordinate of $X | Z_1,\ldots, Z_T$ is a constant biased bit correlated to $X$ (more justification on this towards the end)
	\item We will write $\E_i$ $\Var_i$ to denote expectation under product distribution $\mu_i$.  Let
$$ \gamma_i \defeq \E_i[f] $$
then, $\Var_i[f] = \E_i[f^2] - (\E_i[f])^2 = \gamma_i(1-\gamma_i)$ using the fact that $f$ is boolean, $\E_i [f^2] = \E_i[f] = \gamma_i$.


\item Notice that the $\ell^{th}$ coordinate of  $X| Z_i$ can be sampled as follows:
\begin{align*}
	X^{(\ell)} = \begin{cases}
        Z_i^{(\ell)} & \text{ with prob. } \epsilon\\ 
        \text{ random bit with bias } \mu_i^{(\ell)} & \text{with prob.} 1-\epsilon 
        \end{cases}
 \end{align*}
In other words, $X | Z_i$ is sampled by applying the usual noise operator $T_\epsilon$ under the product distribution $\mu_i$.  So we can write,
$$ \E[f(X)|Z_i] = T_{\epsilon} f (Z_i)$$

\item We can write $f(Y) = \sum_{S \subseteq [n]} \hat{f}_S \chi_S(Y)$ in Efron-Stein decomposition under the product distribution $\mu_i$.  Note that $\hat{f}_{\emptyset} = \E_i [f] = \gamma_i$ and $T_{\epsilon} f = \gamma_i + \sum_{|S| \geq 1} \epsilon^{|S|} \hat{f}_S \chi_S$.

\end{enumerate}

By chain rule of mutual information,

\begin{align*}
	I(f(X): Z_1,\ldots, Z_T) = \sum_{i = 1}^T  I(f(X) : Z_i | Z_1,\ldots, Z_{i-1})
\end{align*}
% BIBLIOGRAPHY

Let $h(p) =  -p \log p - (1-p)\log{(1-p)}   $ denote Shannon entropy function over $[0,1]$
\begin{align*}
I(f(X) : Z_i | Z_1,\ldots, Z_{i-1}) & = H(f(X)|Z_1,\ldots,Z_{i-1}) - E_{Z_i}[ H(f(X)|Z_1,\ldots,Z_i)]\\
 & = h(E_i[f(X)]) - \E_{Z_i}[ h(\E[f(X)|Z_1,\ldots,Z_{i}])]  \\
 & = - \E_{Z_i}\left[ h(\E[f(X)|Z_1,\ldots,Z_{i}]) - h(\E_i[f])\right] \\
 & = - \E_{Z_i}[ h(T_{\epsilon} f(Z_i)) - h(\E_i[f])] \\
	 \end{align*}
Let $h(\gamma_i + \Delta) = h(\gamma_i) + \sum_{t=1}^{\infty} h_t \Delta^t$ denote the Taylor expansion of $h$ around $\gamma_i$.  Substituting we get that,

\begin{align*}
I(f(X) : Z_i | Z_1,\ldots, Z_{i-1}) & = -\sum_{t = 1}^{\infty} h_t \E_{Z_i}[ \Delta(Z_i)^t] 
 \end{align*}
where $\Delta(Z_i) = T_{\epsilon} f(Z_i) - \E_i [f(X)] = \sum_{|S| \geq 1} \epsilon^{S} \hat{f}_S \chi_S(Z_i)$.  The first order term in Taylor series expansion $\E_i [\Delta(Z_i)] = 0$.  So the leading term is the second-order term $- h_2 \E_i[\Delta(Z_i)^2]$ which is of order $O(\epsilon^2)$ since $\E_i[\Delta_i (Z_i)^2] = O(\epsilon^2)$.

\begin{align*}
I(f(X) : Z_i | Z_1,\ldots, Z_{i-1}) & = - \frac{h''(\gamma_i)}{2} \cdot \E_i[\Delta_i (Z_i)^2] + O(\epsilon^3) \\
	& = \frac{1}{2\gamma_i (1-\gamma_i)} \cdot \E_i[\Delta_i (Z_i)^2] + O(\epsilon^3) \qquad \qquad \text{ using } (h''(\gamma_i) = -\frac{1}{\gamma_i(1-\gamma_i)})\\
 & = \frac{\epsilon^2}{2} \cdot \frac{1}{\gamma_i(1-\gamma_i)} \cdot \sum_{|S|=1}  \hat{f}_{S}^2  + O(\epsilon^3) \qquad \qquad \text{ dropping higher powers of $\epsilon$ }\\
 & = \frac{\epsilon^2}{2} \cdot \frac{1}{\Var_i[f]} \cdot \sum_{|S|=1}  \hat{f}_{S}^2  + O(\epsilon^3) \qquad \qquad \text{ surprisingly, $\Var_i[f] = -h''(\gamma_i)$? }\\
  &  = \frac{\epsilon^2}{2} \cdot \frac{\sum_{|S|=1}  \hat{f}_{S}^2 }{\Var_i [f]}  + O(\epsilon^3)
\end{align*} 
Thus each term $I(f(X):Z_i | Z_1,\ldots, Z_{i-1})$ is maximized if $f$ is a dictator, i.e., $\sum_{|S| = 1} \hat{f}_S^2 = \Var_i [f]$.  

Also, if we track any particular bit $f(X) = X^{(\ell)}$, the above calculation shows that after $\Omega(1/\epsilon^2)$ steps, one can expect $X|Z_1,\ldots,Z_t$ be a distribution that is just the $\alpha$-correlated version of $X$ for some $\alpha$.
%
In particular, if we fix $T = C/\epsilon^2$ and send $\epsilon \to 0$.   The distribution of $X| Z_1,\ldots Z_T$ almost surely converges to a distribution that is $\alpha$-correlated with $X$ for some $\alpha$ depending on $C$.  So we get,
$$ \lim_{\epsilon \to 0} I(f(X):Z_1,\ldots,Z_T) \to I(f(X):Y)$$
where $Y$ is $\alpha$-correlated copy of $X$.  

Then, the above calculation shows that term by term, $I(f(X):Z_1,\ldots,Z_i)$ is dominated by dictators.

\fi

\iffalse
\section{Majority-Bit Revealing}

The main issue with the bit-revealing process is that for a bit $x \in \{0,1\}$, the marginal $\E[x|z_1,\ldots,z_T]$ is not concentrated over the choice of $z_1,\ldots,z_T$.  Therefore, when we apply the bit-revealing process on a $n$-bit string $X \in \{0,1\}^n$, in the product distribution $X|Z_1,\ldots,Z_T$ the bits will each have different marginals (a.k.a. noise rates) that vary over the random choice of $Z_1,\ldots,Z_T$.  In the noise operator $T_{1-\alpha}$ each bit must have the same noise rate $\alpha$.

Here is an idea:

Represent a bit as a Majority of a large number $N$ of bits.  While each bit can have different noise rates in the bit-reversal process, the Majority of $N$ such bits has a noise-rate that is concentrated.


Let $g: \{0,1\}^N \to \{0,1\} $ denote the Majority function on $N$ bits.  Given a bit $b \in \{0,1\}$, here is the majority-bit revealing process.
Sample a random $N$-bit string $X \in \{0,1\}^N$ such that $g(X) = b$.  Apply the bit-revealing process to the $N$-bit string to generate $Z_1,\ldots, Z_T \in \{0,1\}^N$.  While each individual bit $X^{(i)}|Z^{(i)}_1,\ldots,Z^{(i)}_t$ can have different noise rates, the noise rate in $g(X)|Z_1,\ldots,Z_T$ would be concentrated.


\begin{lemma}
Let $\Theta$ be a distribution over $[-1,1]$ and let $\overline{\rho} = (\rho_1,\ldots,\rho_N) \in [-1,1]^N$ be drawn i.i.d from $\Theta$.  Let $T_{\overline{\rho}}$ denote the noise operator with $\rho_i$ correlation on bit $i$.
%
Define $S(\overline{\rho}) \defeq \langle g, T_{\overline{\rho}} g \rangle = \sum_{S} \hat{g}_S^2 \prod_{i \in S} \rho_i$.
%
Then the random variable $S(\overline{\rho})$ is concentrated,
\[ \Pr\{ S(\rho) - \bbE[S(\rho))]\}\]
\end{lemma}

\begin{proof}
	Note that the derivative $|D_{\rho_i} (S(\overline{\rho}))| \leq \sum_{S \contain i} \hat{g}_S^2 = \Inf_i(g) = 1/\sqrt{N}$.
	
\end{proof}

\begin{claim}
	For every $\epsilon,\delta > 0$, for all sufficiently large $N$ and for all time $t$,  $\E[g(X)| b, B^{(i)}_1,\ldots, B^{(i)}_t]$ and $\E[g(\tilde{A}^{(i)})| g(A^{(i)} = 0, B^{(i)}_1,\ldots, B^{(i)}_t]$ are concentrated, i.e., 
\end{claim}



\textbf{Notation:}

\begin{itemize}
	\item $A = (A^{(1)},\ldots,A^{(n)}) \in \{0,1\}^{nN}$ is a uniformly random string.  $B_1,\ldots,B_T \in \{0,1\}^{nN}$ are obtained by applying bit-revealing process to each bit independently.

	\item For a function $f: \{0,1\}^n \to \{0,1\}$ and $g \in \{0,1\}^N \to \{0,1\}$, define $f \otimes g: \{0,1\}^{nN} \to \{0,1\}$ in the natural way, $f\otimes g(A) = f( g(A^{(1)}),\ldots,g(A^{(n)}))$

	\item Let $g:\{0,1\}^N \to \{0,1\}$ denote the Majority function on $N$ bits.
\end{itemize}

Here are two claims that we would like to show:
\begin{claim}
	Among all functions $f: \{0,1\}^n \to \{0,1\}$ the following quantity is maximized by $f(x) = x_1$
	\[ I(f \otimes g(A)| B_1,\ldots,B_T)\]
\end{claim}



\begin{enumerate}
	\item Let $x \in \{0,1\}$ and $z_1,\ldots,z_T$  bit-revealing process.  

	Let $\Theta^0_t$ be the distribution of $\E[b|Z_1,\ldots,Z_T] $over $[0,1]$
	\item For each $t$, the law of $X|Z_1,\ldots,Z_t$ is a product distribution for each choice of $Z_1,\ldots, Z_t$.  Over random choice of $b \in \{0,1\}$ and $X, Z_1,\ldots, Z_T$, the marginals $\E[X_i | Z_1,\ldots, Z_t]$ are themselves drawn i.i.d. from a distribution $\Theta_t$ over $[0,1]$.
	\item The marginal of the bit $\E[g(X)|Z_1,\ldots,Z_t]$ is concentrated in that,
\end{enumerate}

\fi

\appendix

% APPENDIX


  


\end{document}
